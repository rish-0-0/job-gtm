"""
Temporal workflow for job listing enrichment
Uses chunked fetching to avoid Temporal gRPC message size limits.
"""
import asyncio
import logging
from datetime import timedelta
from typing import Dict, Any
from temporalio import workflow
from temporalio.common import RetryPolicy

logger = logging.getLogger(__name__)


@workflow.defn
class EnrichmentWorkflow:
    """
    Workflow to enrich all existing job listings with AI-powered insights.
    Uses chunked fetching to keep message sizes under Temporal's 4MB limit.
    """

    @workflow.run
    async def run(
        self,
        chunk_size: int = 100,
        batch_size: int = 100,
        skip_already_enriched: bool = True
    ) -> Dict[str, Any]:
        """
        Execute enrichment workflow:
        1. Get chunk info (count and offsets only - small payload)
        2. For each chunk, fetch jobs and publish to queue
        3. Track progress

        Args:
            chunk_size: Number of jobs to fetch per chunk (keeps gRPC messages small)
            batch_size: Number of jobs to publish per RabbitMQ batch
            skip_already_enriched: Skip jobs already enriched

        Returns:
            Summary of workflow execution
        """
        workflow.logger.info(
            f"[Enrichment Workflow] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        )
        workflow.logger.info(
            f"[Enrichment Workflow] Starting enrichment workflow"
        )
        workflow.logger.info(
            f"[Enrichment Workflow] Config: chunk_size={chunk_size}, batch_size={batch_size}, "
            f"skip_already_enriched={skip_already_enriched}"
        )
        workflow.logger.info(
            f"[Enrichment Workflow] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        )

        # Step 1: Get chunk info (lightweight - just counts and offsets)
        chunk_info = await workflow.execute_activity(
            "get_enrichment_chunk_info",
            args=[chunk_size, skip_already_enriched],
            start_to_close_timeout=timedelta(minutes=2),
            retry_policy=RetryPolicy(
                maximum_attempts=3,
                initial_interval=timedelta(seconds=1),
                maximum_interval=timedelta(seconds=10),
            )
        )

        total_jobs = chunk_info['total_jobs']
        chunks = chunk_info['chunks']

        workflow.logger.info(f"[Enrichment Workflow] ğŸ“Š Found {total_jobs} jobs in {len(chunks)} chunks")

        if total_jobs == 0:
            workflow.logger.info(f"[Enrichment Workflow] âš ï¸ No jobs to enrich, exiting")
            return {
                "total_jobs": 0,
                "published_to_queue": 0,
                "chunk_size": chunk_size,
                "batch_size": batch_size,
                "status": "completed",
                "message": "No jobs to enrich"
            }

        # Step 2: Process each chunk - fetch from DB and publish directly to RabbitMQ
        # This avoids Temporal's gRPC size limit by not returning job data through Temporal
        total_published = 0
        chunks_processed = 0

        for chunk in chunks:
            chunk_index = chunk['chunk_index']
            offset = chunk['offset']
            limit = chunk['limit']

            workflow.logger.info(
                f"[Enrichment Workflow] ğŸ“¥ Processing chunk {chunk_index + 1}/{len(chunks)} "
                f"(offset={offset}, limit={limit})"
            )

            # Fetch and publish in one activity - avoids returning large data through Temporal
            try:
                chunk_published = await workflow.execute_activity(
                    "fetch_and_publish_enrichment_chunk",
                    args=[offset, limit, skip_already_enriched],
                    start_to_close_timeout=timedelta(minutes=5),
                    retry_policy=RetryPolicy(
                        maximum_attempts=3,
                        initial_interval=timedelta(seconds=1),
                        maximum_interval=timedelta(seconds=10),
                    )
                )
                total_published += chunk_published
                chunks_processed += 1

                workflow.logger.info(
                    f"[Enrichment Workflow] âœ… Chunk {chunk_index + 1}/{len(chunks)} done: "
                    f"{chunk_published} published (total: {total_published}/{total_jobs} = {total_published/total_jobs*100:.1f}%)"
                )
            except Exception as e:
                workflow.logger.error(
                    f"[Enrichment Workflow] âŒ Failed chunk {chunk_index + 1}: {str(e)}"
                )
                continue

            # Small delay between chunks
            await asyncio.sleep(0.1)

        workflow.logger.info(
            f"[Enrichment Workflow] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        )
        workflow.logger.info(
            f"[Enrichment Workflow] ğŸ‰ Workflow completed: "
            f"{total_published}/{total_jobs} jobs published ({total_published/total_jobs*100:.1f}%)"
        )
        workflow.logger.info(
            f"[Enrichment Workflow] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        )

        return {
            "total_jobs": total_jobs,
            "published_to_queue": total_published,
            "chunks_processed": chunks_processed,
            "total_chunks": len(chunks),
            "chunk_size": chunk_size,
            "batch_size": batch_size,
            "status": "completed" if total_published == total_jobs else "partial",
            "message": f"Published {total_published} out of {total_jobs} jobs"
        }
